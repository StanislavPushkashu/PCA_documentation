{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is a descriptive method for the multidimensional analysis of data. PCA is applied in the study of the relationship between quantitative variables that are standardized (centered or reduced) for analysis. To calculate the distance between two points is used **euclidean distance**.\n",
    "\n",
    "The results obtained from the application of this method can be used in other statistical analyses, such as regression analysis or in comparing the means of variables by groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is one of the most widely used methods of multidimensional factor analysis. Starting from a large data set, which shows the distribution of some statistical units according to the variation of several numerical variables, $X_{1}$, $X_{2}$, ..., $X_{p}$, PCA highlights a system of factorial axes that conceptualizes the information contained in the initial table for a better visualization.\n",
    "\n",
    "The application of Principal Component Analysis can be realized to achieve the following three major objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. *Description of correlations between variables and similarities/dissimilarities between statistical units*\n",
    "\n",
    "Principal component analysis provides results on statistical variables and results on statistical units. The interpretation of these results covers:\n",
    "- highlighting the statistical relationships (correlations) between the variables. For this purpose, the computer programs provide the graphical representation of the statistical variables in the system of factor axes and present the values of the correlation coefficients between the variables and the respective factor axes.\n",
    "\n",
    "- The statistical units are considered according to the set of recorded variables. To achieve this objective, specialized computer programs provide specific numerical indicators for the statistical units and an easy to interpret graphical representation.\n",
    "\n",
    "- explaining similarities and differences between individuals in terms of the variables considered. To this end, the results obtained for the statistical units are \"correlated\" with the results obtained for the statistical variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. *Selection of the most important variables explaining similarities/differences between statistical units*\n",
    "\n",
    "Principal component analysis is a method of \"*dimension reduction*\", i.e. selecting the variables that explain the most differences between statistical units. The factorial axes (principal components or factors) are presented in descending order, according to the weight of the variance explained by them. Thus, the first component (the factorial axis) *always* explains the largest proportion of the variance explained. The variables that explain the formation of this first component are therefore the variables that *explain the most* similarities/differences between statistical units. In the variable selection approach, the value of the proportion of the variance explained by the first component is important: if the proportion of the variance explained by this first component is small, then the variable selection cannot be realized by considering only the first component "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. *Selection of factors and their use in other statistical analyzes*\n",
    "\n",
    "The results obtained following the application of component analysis by\n",
    "principals can be used in other statistical analyses, as follows:\n",
    "\n",
    "- the factorial axes are linear combinations of the initial variables.\n",
    "The variables that explain the formation of these axes are those variables that have high correlation coefficients (called *factor loadings*) with the respective axes.\n",
    "The selection of these variables makes it possible to \"define\" the axes and use them in other statistical analyses, for example, as predictors in regression analysis. If in a study, we include variables that reflect the economic dimension, the social dimension and the institutional dimension, and the first factor is explained, mainly, by the variables that reflect the economic dimension, then it could be defined as follows (*factor I - economic dimension*).\n",
    "\n",
    "- the factorial axes can be considered new variables, and the \"values\" of these new variables are the coordinates of the statistical units on these axes (*factor scores*). These coordinates can be used in other statistical analyses.\n",
    "For example, if the first factor is called the *economic dimension*, it will have new coordinates for the statistical units. This factor can represent a predictor in a regression analysis, in which the dependent variable is another quantitative variable that was not included in the PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nature of the data\n",
    "\n",
    "The scope of applicability of the PCA method is limited only to quantitative variables, which can be expressed in the same unit of measure or in different units of measure. Before applying PCA, the variables are standardized (all computer programs perform this standardization operation).\n",
    "\n",
    "To be applied effectively, it is recommended to use PCA for a large table, with a number of individuals greater than 15 and a number of variables greater than four (Foucart, 1997). However, these values ​​are indicative, the PCA method can also be applied for smaller tables.\n",
    "\n",
    "The initial data are made up of the observed values ​​of some variables $X_{j}$, with $j=1, p$, for a set of $n$ statistical units. The data table used in PCA is therefore an individuals-variables type table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data standardization\n",
    "\n",
    "Standardization of the data involves the centering and reduction of the variables and consists in the transformation of the $X_{j}$ variables into $X_{j}^{,}$, respectively in the calculation of the $x_{ij}$ values, according to the relationship:\n",
    "\n",
    "$$x_{ij}^, = \\frac{x_{ij} - \\overline{x_{j}}}{s_j}$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\overline{x_{j}} = \\frac{\\sum_{i = 1}^{n} x_{ij}}{n}$ is average of $X_{j}$\n",
    "* $s_{j} = \\sqrt{\\frac{\\sum_{i = 1}^{n}(x_{ij} - \\overline{x_{j}})^{2}}{n}}$ is standard deviation of $X_{j}$\n",
    "\n",
    "The standardization operation of the variables, indispensable in the analysis of the main components when the variables are expressed in different units of measure, has implications in the interpretation of the obtained results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Effects of data standardization**\n",
    "\n",
    "The standardization of variables has the following effects:\n",
    "\n",
    "**1. Countering the cloud of points corresponding to the statistical units\n",
    "what on origin**\n",
    "\n",
    "The standardization of the variables leads to obtaining new variables, $X_{j}^{,}$, with mean 0 and variant 1. The representation of the statistical units that have as coordinates the standardized values ​​of the variables is realized in obtaining a cloud of points centered on the origin. In the analysis of the main components, the origin is, therefore, represented by a point that has as co-ordinates the *average levels* of the considered variables.\n",
    "\n",
    "**2. Loss of part of the initial information**\n",
    "\n",
    "By standardizing the variables, part of the information contained in the initial data table is lost, because, following this operation, the differences between the values ​​of the variables are no longer highlighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Method\n",
    "\n",
    "Starting from the p variables studied, ACP highlights p hierarchical lines, called *factorial axes* or *main components*, on which individuals and variables will be projected, depending on the degree of differentiation between them. These axes, which represent linear combinations of the initial variables, have the advantage of not being correlated with each other, unlike the analyzed variables.\n",
    "\n",
    "In other words, taking into account the fact that the origin in a space with *n* dimensions represents the center of gravity defined by the points whose coordinates are the average values, each point moves away from this center in a certain direction, so that we will obtain a point cloud centered at 0.\n",
    "\n",
    "The direction of the line in which this cloud moves the most away from the center of gravity highlights the dominant tendency characteristic of the analyzed phenomenon.\n",
    "\n",
    "This line represents the *first component (factorial axis)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data analysis through PCA, each component is associated with a part of the information contained in the initial data table. This part does not move *inertia* or *variant explained*. Each of these components (factorial axes) can also be interpreted in terms of correlation with the initial variables. The factorial axes ($F_{j}$) determined by PCA are classified in descending order according to their discriminatory \"power\". Thus, $F_{1}$ differentiates individuals the most from each other, $F_{2}$, less g.a.m.d. The data analysis will thus be limited to the first factorial axes which concretizes a large part of the initial information.Statistical analysis of the data in SPSS and R theirs will thus be limited to the first factor axes that concentrate a large part of the initial information.\n",
    "\n",
    "The search for the components (factorial axes or factors) in whose \"directions\" the *inertia* (variance) between the statistical units is maximum presupposes the *diagonalization* of a symmetric square matrix. A symmetric square matrix is ​​obtained by multiplying the data matrix presented in the initial table, X, with its transpose, $X^{T}$, resulting in the matrix $X^{T}X$ or $XX^{T}$. The eigenvalues ​​of the matrix $X^{T}X$ or $XX^{T}$ represent the variant explained by each factor, and the eigenvectors associated with these values ​​define the factorial axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inertia matrix\n",
    "The inertia matrix n PCA is the correlation matrix.\n",
    "For example, if two variables are recorded, $X_{1}$ and $X_{2}$, the co-relation matrix is of the form:\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{cc}\n",
    "1 & r_{X_{1}X_{2}} \\\\\n",
    "r_{X_{1}X_{2}} & 1\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "where $r_{X_{1}X_{2}}$ is the correlation coefficient\n",
    "between the variables $X_{1}$ and $X_{2}$.\n",
    "The correlation matrix has main diagonal elements equal to one. The sum of these elements measures the total variance which is always in the PCA equal to the number of statistical variables and, implicitly, to the total number of factorial axes.\n",
    "\n",
    "The \"partitioning\" of this total variance on the factorial axes is realized by calculating the eigenvalues (2) of the correlation matrix, using the relation:\n",
    "\n",
    "det $(X^{T}X - \\lambda_{k}I) = 0$.\n",
    "\n",
    "The eigenvalues of the correlation matrix determined after solving the equation defined above measure the variance explained by each component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between statistical units and correlation between variables\n",
    "\n",
    "*In order to assess the similarities or differences* between statistical units in terms of the variables recorded, it is necessary to measure the distances between the points represented by them. To calculate the distance between two statistical units, in PCA, we use the *Euclidean distance*, defined by the relation:\n",
    "\n",
    "$$d(i,i^{,})=\\sqrt{\\sum_{j=1}^{p}(x_{ij}-x_{i^{,}j})^{2}}$$\n",
    "\n",
    "where: d(i,i') is the distance between the statistical units i and i'; \n",
    "\n",
    "$x_{ij}$, is the value of the variable $X_{j}$, observed for individual i; \n",
    "\n",
    "$x_{i^{,}j}$ is the value of the variable $X_{j}$, observed for individual i.\n",
    "\n",
    "*For analyzing the relationship between two variables* ($X_{j}, X_{j^{,}}$), the correlation coefficient between them is calculated according to the relation:\n",
    "\n",
    "$$r_{X_{j}X_{j^{,}}}=\\frac{cov(X_{j},X_{j^{,}})}{s_{j}s_{j^{,}}}$$\n",
    "\n",
    "where: cov(X, X,) is the covariance between the variables $X_{j}$, and $X_{j^{,}}$; \n",
    "\n",
    "$s_{j}$, $s_{j^{,}}$, are the standard deviations of the variables $X_{j}$, and $X_{j^{,}}$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud representation of statistical units\n",
    "\n",
    "Each statistical unit can be represented as a point in j-dimensional vector space. The set of statistical units constitutes the point cloud having as center of gravity (G) the origin of the axes.\n",
    "\n",
    "In this space, the distance between two units is the Euclidean distance.\n",
    "The units that are most similar to each other are those for which the distance between them is minimal. Analyzing the shape of the point cloud involves finding the direction of maximum elongation of the cloud, and this defines the first factorial axis. Representation of the cloud of statistical units in a space of dimension greater than three (for a number of variables greater than three) is impossible. The advantage of factor analysis methods lies in the fact that it is possible to obtain graphical representations in the space of different factorial axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud representation of the variables under study\n",
    "\n",
    "Each variable $X_{j}^{'}$, can be represented as a vector in a vector space of i dimensions, where each dimension constitutes a statistical unit.\n",
    "\n",
    "The vectors representing the variables X'; are normed vectors (of norm 1), i.e:\n",
    "\n",
    "$$\\left\\lvert\\left\\lvert X_{j}^{,}\\right\\rvert\\right\\rvert^{2} = \\sum_{i = 1}^{n}(\\frac{x_{ij}-\\overline{x_{j}}}{s_{j}})^{2} = 1$$\n",
    "\n",
    "The point cloud $X_{j}^{,}$; is located on a hypersphere of radius 1. The position of the variables will be interpreted by their position with respect to the correlation circle of radius l and not by their distance from the center of gravity (the points represented by the variables are at the same distance from the origin because the variance of the standardized variables is equal to one).\n",
    "\n",
    "Cosine of the angle formed by vectors representing two variables, $X_{1}^{,}$ and $X_{2}^{,}$, is obtained by making the scalar product between them, denoted ($X_{1}^{,}$, $X_{2}^{,}$) . Considering the angle 0 between two vectors $X_{1}^{,}$, $X_{2}^{,}$, their scalar product is equal to the product of their length and the cosine of the angle between them:\n",
    "\n",
    "$$(X_{1}^{,} * X_{1}^{,}) = cos\\phi * \\left\\lvert\\left\\lvert X_{1}^{,}\\right\\rvert\\right\\rvert * \\left\\lvert\\left\\lvert X_{2}^{,}\\right\\rvert\\right\\rvert$$\n",
    "\n",
    "The cosine of angle $\\phi$ is therefore the scalar product of two vectors normalized vectors, as follows:\n",
    "\n",
    "$$\n",
    "cos\\phi = \\frac{(X_{1}^{,} * X_{2}^{,})}{\\left\\lvert\\left\\lvert X_{1}^{,}\\right\\rvert\\right\\rvert * \\left\\lvert\\left\\lvert X_{2}^{,}\\right\\rvert\\right\\rvert} \n",
    "= \n",
    "\\frac{\\sum_{i = 1}^{n}x_{i1}^{,}x_{i2}^{,}}{\\sqrt{\\sum_{i = 1}^{n}x_{i1}^{,^{2}}}\\sqrt{\\sum_{i = 1}^{n}x_{i2}^{,^{2}}}}\n",
    "$$\n",
    "\n",
    "In case of standardized variables:\n",
    "\n",
    "$$\n",
    "cos\\phi = \\frac{(X_{1}^{,} * X_{2}^{,})}{\\left\\lvert\\left\\lvert X_{1}^{,}\\right\\rvert\\right\\rvert * \\left\\lvert\\left\\lvert X_{2}^{,}\\right\\rvert\\right\\rvert} \n",
    "= \n",
    "\\sum_{i=1}^{n}\\frac{1}{n}(\\frac{x_{i1}-\\overline{x_{1}}}{s_{1}})(\\frac{x_{i2}-\\overline{x_{2}}}{s_{2}})\n",
    "=\n",
    "\\frac{cov(X_{1}^{,},X_{2}^{,})}{s_{1}s_{2}}\n",
    "=\n",
    "r_{X_{1}^{,},X_{2}^{,}}\n",
    "$$\n",
    "\n",
    "Thus, the correlation coefficient between two standardized variables is the *cosine of the angle between the vectors that have as coordinates these centered and reduced values* (cos$\\phi$). Therefore, the interpretation of the correlation between variables requires the analysis of the angle formed by the vectors that define the respective variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the first component\n",
    "\n",
    "Principal component analysis involves the identification of factor axes (principal components) that represent a linear combination of\n",
    "those variables that are correlated with each other.\n",
    "The first component (Fi) corresponds to the line closest to all points of the cloud, using the least squares criterion. It can be defined as follows:\n",
    "\n",
    "F1 = a1 - X1 + a2 - X2+... ap - Xp\n",
    "\n",
    "The variance or inertia explained by the first principal component represents the largest eigenvalue of the correlation matrix, $\\lambda_{k}$. The eigenvalues are ordered in descending order and their sum is equal to the number of initial variables. This sum measures the total variance of the point cloud.\n",
    "\n",
    "The principal vectors corresponding to the factorial axes are the eigenvectors of the correlation matrix, associated with the eigenvalues $\\lambda_{k}$. These vectors are unitary (the sum of squares of its components is equal to 1) and orthogonal (their scalar product is zero).\n",
    "\n",
    "The eigenvector associated with the largest eigenvalue of the co-relatile matrix is the vector that defines the maximum elongation direction of the point cloud, i.e. the first component. This vector represents a new variable which is a linear combination of the initial variables.\n",
    "The coordinates of this eigenvector are used to find the coefficients associated to each variable considered in the equation defined by the first component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis in R\n",
    "PCA consists in determining the eigenvectors and eigenvalues of the matrix of correlations between statistical variables. All these calculations are performed by computer on the basis of specialized software (R) which facilitates the analysis of data tables of important dimensions.\n",
    "\n",
    "Depending on the objective pursued by applying principal component analysis (analysis of correlations between variables, analysis of the position of individuals in the sample in terms of the variables considered), the method approach leads to the results on statistical variables and results on statistical units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection and importance ranking of active and additional variables\n",
    "\n",
    "The selection and ranking of the importance of active and additional variables that contribute to the formation of the extracted components can be realized in R, using the *dimdesc* function in the *FactoMineR* package.\n",
    "\n",
    "**Selection and importance ranking of quantitative variables**\n",
    "\n",
    "*The selection of the quantitative variables* is based on the co-relation coefficients between a variable and the extracted component (*factor loadings*): only *variables that are correlated with the respective component* (with the extracted component) and that are statistically significant for an $\\alpha$ assumed risk (usually the risk is 0.05) are selected.\n",
    "\n",
    "*The ranking of the importance of quantitative variables is realized according to the value of the correlation coefficient* between a variable and the extracted component (variables are presented hierarchically, from the highest positive value to the lowest negative value). The hierarchy of the importance of the variables in the formation of the components is given by the value of these coefficients in absolute magnitude.\n",
    "\n",
    "In the tables of results, only the quantitative variables that show a statistically semi-significant influence on the component formation are presented.\n",
    "\n",
    "**Selection and importance ranking of qualitative variables and their categories**\n",
    "\n",
    "*The selection of the qualitative variables* is made on the basis of the group mean difference test: the grouping variable is the qualitative variable, and the quantitative variable used in the test is represented by the coordinates on the extracted components (*factor scores*), Thus, following this statistical test, only the qualitative variables that have a semi-figurative influence on the extracted components are selected.\n",
    "\n",
    "*The selection of the categories of the qualitative variables*, which explain the formation of the components, is carried out by a one-factor analysis of variance model, in which the dependent variable is represented by the coordinates of the indi-vices on the extracted component (quantitative variable), and the independent variable is the category of the qualitative variable.\n",
    "\n",
    "In the results tables, only the qualitative variables and their categories that have a statistically significant influence on the shape of the components are presented."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "R"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
